# Varp Architecture

*From the Design Document — v0.1.0 — February 2026*

See [Design Principles](design-principles.md) for the foundational concepts (static/dynamic, 3D model, tiered knowledge, supervision tree).

## 3. Architecture

Varp has three components that evolve independently:

- The **component manifest** — declarative state of the system (what exists)
- The **plan** — declarative intent for change (what should happen)
- The **orchestrator** — adaptive execution engine (how it happens)

This separation mirrors proven systems architecture. The manifest is the database schema. The plan is the transaction. The orchestrator is the transaction manager and query planner. Each has a different rate of change: the manifest changes with project structure (slow), plans change with features (medium), and orchestrator strategy can change per-session (fast). Matching rate of change to separation boundary is what makes the system maintainable.

### 3.1 Component Manifest

The manifest is a YAML file that maps the project's module structure. It is the persistent, human-maintained source of truth about what exists and where it's documented.

```yaml
varp: 0.1.0

auth:
  path: ./src/auth
  docs:
    - ./docs/auth/README.md
    - ./docs/auth/internal.md

api:
  path: ./src/api
  deps: [auth]
  docs:
    - ./docs/api/README.md
    - ./docs/api/internal.md

web:
  path: ./src/web
  deps: [auth, api]
  docs:
    - ./docs/web/README.md
    - ./docs/web/internal.md
```

**The manifest is flat.** The `varp` key holds the version string. Every other top-level key is a component name. No `components:` wrapper, no `name:` field. Docs are plain string paths, not objects.

**Doc visibility uses the README.md convention.** Docs named `README.md` are public — loaded when a task reads from or writes to the component. All other docs are private — loaded only when a task writes. This replaces the previous `load_on` tag system. Auto-discovery: if `{component.path}/README.md` exists on disk, it's included automatically as a public doc.

**Components include everything that gets modified as a unit.** Source code, skills, hooks — anything an agent might need context to work on. A skill component's "source" is its SKILL.md; its docs might be the design doc section describing that workflow.

**Dependencies** declare the static relationship between components. `web` depends on `auth` and `api`, meaning it consumes their interfaces. This serves two purposes: the planner agent uses it to understand the project's dependency graph when decomposing features, and the framework uses it for cross-session invalidation — when `auth`'s docs change, any component that depends on `auth` is flagged for review even if no plan is currently active.

Task-level `touches` declarations are the per-operation subset of these static dependencies. A task on `web` that reads from `auth` is consistent with `web`'s `deps: [auth]`, but not every task on `web` will touch every dependency. The manifest captures the structural truth; `touches` captures the operational specifics.

**Varp tracks architectural dependencies, not package dependencies.** `deps` captures behavioral relationships between components — "web consumes auth's interface and assumes certain behaviors." Package-level dependencies (shared libraries, framework versions, transitive npm dependencies) are deferred to existing tooling. Monorepo tools like Turborepo, pnpm workspaces, and nx already maintain the package dependency graph, resolve transitive dependencies, and can report impact radius when shared dependencies change. The orchestrator and planner agent should query these tools (e.g., `turbo ls --affected`, `pnpm why react`) when planning work that involves shared dependency upgrades, rather than maintaining a parallel dependency graph in the manifest.

When the orchestrator dispatches a task that `writes` to `auth` and `reads` from `api`, it resolves docs via the README.md convention: auth's README.md and internal docs both load (full context for modification), while api's README.md loads (public interface only, enough to use correctly). The plan references components by name, the manifest resolves names to file paths and documentation.

**The README.md IS the contract.** There is no separate contract artifact. A component's `README.md` describes its API surface, behavioral assumptions ("this module assumes the request has already been authenticated"), ordering guarantees, and what it explicitly does not guarantee. This is the single source of truth for how other components and agents should interact with it.

Some of this content can be autogenerated (type exports, route definitions), but the behavioral assumptions that actually break agent work must be written by humans, because no tool can extract semantic expectations from code. The README.md is where both live, in whatever mix is appropriate for the component.

### 3.2 Plan Format

Plans are XML files that declare what should change. They are produced by a **planner agent** — a specialized agent whose domain is decomposing vague human intent into concrete, verifiable plans. The human describes what they want ("add rate limiting to auth endpoints"), the planner asks clarifying questions ("per-user or per-IP? what's the threshold? what HTTP status on limit?"), and produces a plan with Hoare Logic contracts that the orchestrator can execute autonomously.

The planner and orchestrator never run simultaneously. The planner session is a conversation between human and planning agent that produces `plan.xml` as its artifact. The orchestrator session picks up that artifact and executes it. They communicate through the plan file, not through shared context.

**Tasks are scoped by component, not by action type.** A single task covers implementation, tests, and doc updates for its component scope. You split tasks when scope would overflow a context window, not when the action type changes. Tests are part of the scope — a task writing to `auth` includes writing auth's tests because they need the same context and verify the same behavior.

**The plan does not specify execution order.** Tasks declare their read and write sets via `touches`. The orchestrator derives execution order at runtime by analyzing data dependencies. This separation means plans can be written by a planner agent (or human) without needing to reason about scheduling, and the orchestrator can optimize execution independently.

**Two plan modes coexist within the same schema:**

- **Directed plans** include explicit action steps within tasks. The agent follows them. Appropriate for simpler, well-understood work.
- **Contract plans** include only postconditions. The agent determines its own path to satisfaction. Appropriate for complex autonomous work.

**Why XML:** Reliable agent parsing, clear nesting for structured data (tasks within plans, conditions within contracts), and schema validation. Markdown is preferred for narrative documentation; XML is preferred for machine-consumed control structures.

**Verification is test-driven.** Postconditions should be test suites, not bespoke shell commands. The planner writes test expectations ("rate limiting returns 429 after threshold"), the subagent implements TDD (write failing tests, implement until they pass), and the orchestrator verifies at wave boundaries by running the test suite. Invariants remain as the broader regression check ("full test suite still passes, types compile"). Each `<verify>` element is a shell command that exits 0 on success — prefer `bun test --filter=rate-limit` over `grep -r "router\."`.

**Resource budgets are per-task.** Each task declares token and time limits that the orchestrator enforces during execution. Directed tasks get tighter budgets (the path is known). Contract tasks get more headroom (the agent must discover its own approach). The planner sets initial budgets based on task complexity; the orchestrator can adjust based on execution history from previous waves or sessions.

#### 3.2.1 The Planner Agent

The planner agent is a specialized agent whose domain is decomposing vague human intent into concrete, verifiable plans. It runs in a dedicated session — a conversation between human and planner — and produces `plan.xml` as its artifact.

**Planner protocol:**

1. **Load manifest** — read `varp.yaml` to understand component structure and dependency graph
2. **Clarify intent** — ask the human targeted questions to resolve ambiguity ("per-user or per-IP? what HTTP status on limit?")
3. **Decompose** — break the feature into tasks scoped to individual components. Each task covers implementation, tests, and doc updates for its scope. Split by context window pressure, not by action type.
4. **Derive `touches`** — for each task, determine which components it reads from and writes to, cross-referencing the manifest's `deps` graph for consistency. Two tasks writing the same component is a plan smell — merge them.
5. **Set budgets** — assign token and time limits per task, scaled to scope complexity (budgets include implementation + tests + docs)
6. **Write contracts** — preconditions (readiness), invariants (regression checks at wave boundaries), and postconditions (test suites that verify the feature works). Prefer test-driven postconditions over bespoke shell scripts.
7. **Choose plan mode** — directed (explicit steps) for well-understood work, contract (postconditions only) for complex autonomous work
8. **Output `plan.xml`** — the complete plan artifact

**`touches` validation is the planner's responsibility.** The entire concurrency model depends on correct read/write declarations. The planner derives `touches` from both the task description and the manifest's dependency graph. If a task on `web` needs to change behavior that flows through `auth`, that's a write to `auth`, not just `web` — even if the files being edited are in `web`'s directory. The planner must reason about behavioral dependencies, not just file locations.

The orchestrator performs a consistency check at dispatch time: if a task's `touches` references a component not in the manifest, or if a write target isn't reachable through `deps`, execution halts and kicks back to replanning. This catches the most obvious errors but cannot catch undeclared dependencies — those surface as postcondition failures after execution.

### 3.3 The Orchestrator

The orchestrator is Claude Code — specifically, a Claude Code session with Varp's MCP tools and skills loaded. It is both coordinator and process manager: it maintains the accurate project model, translates architectural intent into concrete subagent prompts, schedules and supervises task execution, enforces resource budgets, handles failures, and executes the plan's task graph.

Varp does not implement its own orchestrator runtime. Claude Code already provides the agent loop, tool execution, subagent dispatch (Task tool), session management, hooks system, and context compaction. Varp adds manifest-awareness to this existing infrastructure through MCP tools that the orchestrator calls during its work cycle.

**Execution modes.** The orchestrator classifies the plan's scope shape at initialization and adapts its protocol accordingly:

- **Single-scope** — all tasks write to the same component. Skip wave computation; execute sequentially. Most plans on small projects use this mode.
- **Sequential multi-scope** — tasks write to different components with RAW dependencies. Compute waves but expect one task per wave. Pass observations forward.
- **Parallel multi-scope** — tasks write to different components with independent waves. Full wave computation, critical path scheduling, parallel dispatch.

**Enforced chain of thought:** The orchestrator follows this protocol for each work cycle. Steps are conditional on execution mode — some are skipped in simpler modes.

1. **Select** — pick the next executable task(s). Single-scope: next by ID. Parallel: tasks whose RAW deps are satisfied, prioritizing critical path.
2. **Verify** — check preconditions (via plan's `<verify>` commands)
3. **Resolve** — resolve doc paths via `varp_resolve_docs`. The orchestrator resolves paths; the subagent reads content. The orchestrator should not load component internals into its own context.
4. **Dispatch** — send task to subagent with assembled context, capability grants, and postconditions. The subagent owns implementation + tests + docs for its scope.
5. **Collect** — receive structured result (`COMPLETE|PARTIAL|BLOCKED|NEEDS_REPLAN`) and record in log.xml
6. **Verify capabilities** — confirm actual file changes match declared `touches` (via `varp_verify_capabilities`)
7. **Verify invariants** — run invariant checks at wave boundaries (not between individual tasks). Tests may break mid-task during schema changes — the invariant holds at wave boundaries.
8. **Handle failure** — derive restart strategy from `touches` graph (via `varp_derive_restart_strategy`)
9. **Invalidate** — cascade changes to dependent contexts (via `varp_invalidation_cascade`). Parallel mode only.
10. **Advance** — mark task complete, unblock dependent tasks

**Wave cancellation.** If a critical invariant fails during a wave (checked between task completions), the orchestrator signals all running subagents in the wave to stop. Graceful cancellation lets subagents finish their current atomic operation and return partial results. Hard cancellation abandons worktrees and discards everything. The orchestrator chooses based on invariant severity: `critical="true"` invariants trigger hard cancellation; others allow graceful wind-down.

**Capability enforcement.** `touches` declarations are not just scheduling hints — they are capability grants. After a subagent completes, the orchestrator verifies that actual file modifications fall within the declared write set by checking the git diff against component path boundaries from the manifest. A subagent dispatched with `touches writes="auth"` that modified files in `web/`'s path has violated its capabilities. This is caught before merge, not after — the worktree is quarantined and the task either retried with corrected scope or escalated to replanning.

**Subagents are processes with functional interfaces.** They are *defined* functionally — domain, values, action, context — but *execute* as processes with lifecycles, resource consumption, and structured exit states. The orchestrator resolves doc paths and passes them to the subagent; the subagent reads the actual content. This keeps the orchestrator lean (meta-level only) while giving subagents full component context. Subagents own their entire scope — implementation, tests, and doc updates — and return a structured result. They don't know about other agents and don't manage state beyond their current session.

"Warm agents" are resumed subagent sessions — when a later task operates on the same component scope, the orchestrator can resume the previous subagent's session rather than starting cold, preserving the subagent's accumulated context about that component. This is analogous to fork-and-COW: the new task starts with the previous subagent's component knowledge (shared pages), then diverges only where the new work requires it (copy-on-write). This is distinct from orchestrator session persistence: the orchestrator session lives across the whole plan, while subagent sessions are per-component and optionally resumed.

### 3.4 Delivery: Claude Code Plugin

Varp is delivered as a Claude Code plugin, not a standalone CLI. The plugin provides three integration layers:

**MCP tools (core logic):** Deterministic functions exposed as MCP tools that the orchestrator calls during its work cycle. These handle manifest parsing, doc resolution, hazard detection, wave computation, invalidation cascading, capability verification, and resource tracking — the mechanical operations that should be code, not agent reasoning.

**Skills (workflow protocols):** Slash commands that load the appropriate protocol and context for each Varp workflow. Skills are reusable — they structure the agent's behavior for a session without persisting as permanent system prompts. When a skill is invoked, it loads its protocol; when the session ends, the protocol is unloaded. A normal Claude Code session has no Varp overhead.

- `/varp:plan` loads the planner protocol and the manifest, turning the Claude Code session into a planning conversation.
- `/varp:execute` loads the orchestrator protocol and the active plan, turning the session into an execution run.
- `/varp:review` surfaces the medium loop decision surface — the log.xml diffed against the plan's expected outcomes.
- `/varp:status` reports project state: component registry, doc freshness, plan lifecycle status, dependency graph health.

The tiered knowledge architecture is implemented through skill loading, not through persistent configuration. Each skill loads exactly the T1 context its workflow needs. T2 knowledge (component docs) is loaded dynamically by MCP tools during execution, resolved from manifest references.

**Hooks (enforcement):** Lifecycle hooks that enforce Varp conventions during active Varp sessions. `SubagentStart` auto-injects relevant component docs based on the orchestrator's resolved context for the current task. `PostToolUse` flags docs for refresh after file writes. `SessionStart` loads the manifest and displays project state when a Varp skill is invoked.

**Prompt caching integration:** The Anthropic SDK's prompt caching (90% cost reduction on cache reads) maps naturally to Varp's tiered knowledge. T1 knowledge (manifest + skill protocol) is cached at the system prompt level — stable across all dispatches within a session. T2 knowledge (component docs) is cached per component scope — reused across tasks that share the same component context. Cache breakpoints are placed at tier boundaries: tools → system (T1) → component docs (T2) → task-specific context.

## 4. Concurrency Model

### 4.1 The DBMS Analogy

Varp's concurrency model borrows directly from database management systems because it solves the same fundamental problem: multiple actors changing shared state safely.

| DBMS Concept | Varp Equivalent |
|---|---|
| Schema | Component manifest |
| Transaction | Plan (set of tasks) |
| Transaction manager | Orchestrator |
| MVCC / Write-ahead log | Git branches and worktrees |
| Materialized views | Component documentation (interface + internal) |
| Constraint checking | Postcondition verification |
| View maintenance | Invalidation cascade |

The critical insight is that Varp doesn't care what your software project *does*, just as a DBMS doesn't care what data *means*. Varp manages component structure, documentation consistency, and work concurrency generically. The domain knowledge lives in the plans and docs, not in the framework.

### 4.2 Data Hazards

Tasks that operate on shared components create the same data hazards as concurrent memory operations in CPU pipelines:

**RAW (Read After Write) — true dependency.** Task B reads a component that Task A writes. Task B must wait for Task A to complete. This is the only hard scheduling constraint.

**WAR (Write After Read) — anti-dependency.** Task A reads a component that Task B writes. If Task B runs first, Task A reads the wrong state. Resolved by context snapshotting: the orchestrator captures component documentation at dispatch time, so the reader's context is frozen regardless of subsequent writes. This is register renaming.

**WAW (Write After Write) — output dependency.** Tasks A and B both write to the same component. The last writer wins, so order matters. This is either a scheduling constraint or a plan design smell — two tasks writing the same component usually indicates they should be merged or sequenced intentionally.

### 4.3 Git as MVCC

Git worktrees implement multiversion concurrency control naturally:

**Each parallel task gets its own worktree** branched from the same commit (HEAD). This is snapshot isolation — each task sees a consistent view of the codebase as it existed when the task started.

**WAR is resolved automatically.** The reader's worktree contains the pre-write state. No coordination needed between concurrent readers and writers.

**Merge is the commit operation.** When tasks complete, the orchestrator merges their worktrees back to main. Merge order follows dependency constraints (RAW targets first).

**Merge conflicts are runtime WAW detection.** If two tasks modified the same files, git reports exactly where. This is conflict detection that's both free and more precise than any hand-written checker.

**Rollback is trivial.** If a task fails, delete its worktree. Main is untouched. This is transaction abort with zero cost.

### 4.4 Pessimistic Scheduling, Optimistic Execution

The concurrency control strategy is a hybrid informed by the specific cost structure of agent work:

**Agent transactions are expensive to abort.** A failed task may represent minutes of work and thousands of tokens. Unlike database transactions that can be cheaply retried, agent tasks are nondeterministic — re-running doesn't guarantee the same result.

**Conflicts are preventable at planning time.** The `touches` metadata on each task declares read and write sets upfront. The orchestrator can analyze all hazards before dispatching any work.

Therefore: **prevent conflicts at scheduling time (pessimistic), execute freely within worktrees (optimistic).**

The orchestrator analyzes the task graph's `touches` declarations, identifies RAW/WAR/WAW hazards, and groups tasks into execution waves where no two concurrent tasks write to the same component. Within a wave, each task runs in its own worktree without interference. Between waves, the orchestrator merges results, verifies invariants, and dispatches the next wave.

**Critical path scheduling.** Not all tasks are equally urgent. Tasks on the critical path — those whose completion unblocks the most downstream work — are prioritized within each wave. The critical path is derivable from the dependency graph: it's the longest chain of RAW dependencies from any root task to any leaf task. The orchestrator computes this at plan load time and uses it to prioritize dispatch order when multiple tasks are eligible.

### 4.5 Verification as Serialization Check

Snapshot isolation has a known weakness: write skew. Two tasks can read overlapping state, make individually valid but collectively inconsistent changes to disjoint components, and both succeed. Git merges cleanly because the changes don't overlap textually, but they may conflict semantically.

Postcondition verification on the merged result is the serialization check. After all tasks in a wave complete and merge, the orchestrator runs invariant checks and postconditions on the integrated state. If these fail, the human reviews the conflict — automatic retry is not appropriate because agent tasks are nondeterministic.

This is equivalent to Serializable Snapshot Isolation (SSI) in PostgreSQL, but with human-in-the-loop conflict resolution instead of automatic transaction abort.
